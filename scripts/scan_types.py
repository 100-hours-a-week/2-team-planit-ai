"""
VectorDBì— ì €ì¥ëœ POIë“¤ì˜ types ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸

ê¸°ì¡´ ChromaDBì— ì €ì¥ëœ ëª¨ë“  POIì˜ types í•„ë“œë¥¼ ìŠ¤ìº”í•˜ì—¬:
1. unique type ëª©ë¡ + ë¹ˆë„ìˆ˜
2. typeë³„ POI ìˆ˜
3. ì—¬í–‰ ê´€ë ¨/ë¬´ê´€ íƒ€ì… ë¶„ë¥˜
ê²°ê³¼ë¥¼ ì½˜ì†” + JSON íŒŒì¼ë¡œ ì¶œë ¥í•©ë‹ˆë‹¤.
"""

import json
import os
import sys
from collections import Counter, defaultdict
from pathlib import Path

import chromadb
from chromadb.config import Settings

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ PYTHONPATHì— ì¶”ê°€
PROJECT_ROOT = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

# ======= ì„¤ì • =======
VECTOR_DB_DIR = PROJECT_ROOT / "app" / "data" / "vector_db"
COLLECTION_NAME = "poi_embeddings"
OUTPUT_DIR = PROJECT_ROOT / "results"
OUTPUT_FILE = OUTPUT_DIR / "type_analysis.json"

# ì—¬í–‰ ì¶”ì²œì— ë¶€ì í•©í•œ íƒ€ì… (ì œì™¸ ëŒ€ìƒ)
EXCLUDED_TYPES = {
    # Table B ë©”íƒ€ íƒ€ì…
    "geocode",
    # Transportation (ëŒ€ë¶€ë¶„)
    "airport", "airstrip", "bus_station", "bus_stop",
    "ferry_terminal", "heliport", "international_airport",
    "light_rail_station", "park_and_ride", "subway_station",
    "taxi_stand", "train_station", "transit_depot",
    "transit_station", "truck_stop",
}


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print(f"VectorDB ê²½ë¡œ: {VECTOR_DB_DIR}")
    print(f"ì»¬ë ‰ì…˜ ì´ë¦„: {COLLECTION_NAME}")
    print("=" * 80)

    # 1. ChromaDB ì—°ê²°
    if not VECTOR_DB_DIR.exists():
        print(f"âŒ VectorDB ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {VECTOR_DB_DIR}")
        return

    client = chromadb.PersistentClient(
        path=str(VECTOR_DB_DIR),
        settings=Settings(anonymized_telemetry=False)
    )

    try:
        collection = client.get_collection(name=COLLECTION_NAME)
    except Exception as e:
        print(f"âŒ ì»¬ë ‰ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
        print(f"ì¡´ì¬í•˜ëŠ” ì»¬ë ‰ì…˜: {[c.name for c in client.list_collections()]}")
        return

    total_count = collection.count()
    print(f"ì´ POI ìˆ˜: {total_count}")
    print()

    if total_count == 0:
        print("âš ï¸ ì»¬ë ‰ì…˜ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    # 2. ëª¨ë“  ë©”íƒ€ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (ë°°ì¹˜ë¡œ)
    BATCH_SIZE = 1000
    all_type_counter = Counter()       # type â†’ ì „ì²´ ë¹ˆë„
    type_to_pois = defaultdict(list)   # type â†’ [poi_id, ...]
    poi_type_counts = []               # POIë³„ íƒ€ì… ìˆ˜ ë¦¬ìŠ¤íŠ¸
    pois_without_types = 0

    for offset in range(0, total_count, BATCH_SIZE):
        results = collection.get(
            offset=offset,
            limit=BATCH_SIZE,
            include=["metadatas"]
        )
        ids = results["ids"]
        metadatas = results["metadatas"]

        for doc_id, metadata in zip(ids, metadatas):
            types_raw = metadata.get("types", "[]")
            try:
                types = json.loads(types_raw) if types_raw else []
            except (json.JSONDecodeError, TypeError):
                types = []

            if not types:
                pois_without_types += 1
                poi_type_counts.append(0)
                continue

            poi_type_counts.append(len(types))
            for t in types:
                all_type_counter[t] += 1
                type_to_pois[t].append(doc_id)

    # 3. ë¶„ì„ ê²°ê³¼ ì¶œë ¥
    print(f"ğŸ“Š íƒ€ì… ë¶„ì„ ê²°ê³¼")
    print("=" * 80)
    print(f"ì´ POI ìˆ˜: {total_count}")
    print(f"íƒ€ì…ì´ ì—†ëŠ” POI ìˆ˜: {pois_without_types}")
    print(f"ì „ì²´ unique íƒ€ì… ìˆ˜: {len(all_type_counter)}")
    print()

    if poi_type_counts:
        avg_types = sum(poi_type_counts) / len(poi_type_counts)
        max_types = max(poi_type_counts)
        print(f"POIë‹¹ í‰ê·  íƒ€ì… ìˆ˜: {avg_types:.1f}")
        print(f"POIë‹¹ ìµœëŒ€ íƒ€ì… ìˆ˜: {max_types}")
        print()

    # ì—¬í–‰ ê´€ë ¨ vs ì œì™¸ íƒ€ì… ë¶„ë¥˜
    travel_types = {}
    excluded_found = {}
    for type_name, count in all_type_counter.most_common():
        if type_name in EXCLUDED_TYPES:
            excluded_found[type_name] = {
                "count": count,
                "poi_count": len(type_to_pois[type_name])
            }
        else:
            travel_types[type_name] = {
                "count": count,
                "poi_count": len(type_to_pois[type_name])
            }

    print(f"âœ… ì—¬í–‰ ê´€ë ¨ íƒ€ì…: {len(travel_types)}ê°œ")
    print("-" * 60)
    for type_name, info in sorted(travel_types.items(), key=lambda x: -x[1]["count"]):
        print(f"  {type_name:40s}  ë¹ˆë„: {info['count']:4d}  POI: {info['poi_count']:4d}ê°œ")

    print()
    print(f"âŒ ì œì™¸ íƒ€ì… (EXCLUDED_TYPESì— í•´ë‹¹): {len(excluded_found)}ê°œ")
    print("-" * 60)
    for type_name, info in sorted(excluded_found.items(), key=lambda x: -x[1]["count"]):
        print(f"  {type_name:40s}  ë¹ˆë„: {info['count']:4d}  POI: {info['poi_count']:4d}ê°œ")

    # 4. JSON ì €ì¥
    output = {
        "summary": {
            "total_pois": total_count,
            "pois_without_types": pois_without_types,
            "total_unique_types": len(all_type_counter),
            "travel_related_types": len(travel_types),
            "excluded_types": len(excluded_found),
            "avg_types_per_poi": round(avg_types, 2) if poi_type_counts else 0,
        },
        "travel_types": {
            k: v for k, v in sorted(travel_types.items(), key=lambda x: -x[1]["count"])
        },
        "excluded_types_found": {
            k: v for k, v in sorted(excluded_found.items(), key=lambda x: -x[1]["count"])
        },
        "all_types_frequency": dict(all_type_counter.most_common()),
    }

    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        json.dump(output, f, ensure_ascii=False, indent=2)

    print()
    print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥: {OUTPUT_FILE}")


if __name__ == "__main__":
    main()

from typing import Dict, List, Optional, Any
import asyncio
import json
import logging
import re
from datetime import datetime
from pathlib import Path
from langgraph.graph import StateGraph, END
from app.core.models.PoiAgentDataclass.poi import (
    PoiAgentState,
    PoiSearchResult,
    PoiInfo,
    PoiData,
    PoiValidationError,
    PoiSearchStats,
    _convert_poi_info_to_data,
)

logger = logging.getLogger(__name__)
from app.core.Agents.Poi.PoiMapper.BasePoiMapper import BasePoiMapper
from app.core.Agents.Poi.PoiMapper.GoogleMapsPoiMapper import GoogleMapsPoiMapper
from app.core.LLMClient.BaseLlmClient import BaseLLMClient
from app.core.Agents.Poi.QueryExtention.QueryExtention import QueryExtension
from app.core.Agents.Poi.WebSearch.WebSearchAgent import WebSearchAgent
from app.core.Agents.Poi.VectorDB.BaseVectorSearchAgent import BaseVectorSearchAgent
from app.core.Agents.Poi.VectorDB.VectorSearchAgent import VectorSearchAgent
from app.core.Agents.Poi.ResultMerger import ResultMerger
from app.core.Agents.Poi.InfoSummaizeAgent import InfoSummarizeAgent
from app.core.Agents.Poi.VectorDB.EmbeddingPipeline.EmbeddingPipeline import EmbeddingPipeline
from app.core.Agents.Poi.Reranker.Reranker import Reranker
from app.core.Agents.Poi.PoiAliasCache import PoiAliasCache
from app.core.Agents.Poi.WebSearch.Extractor.JinaReader import JinaReader
from app.core.Agents.Poi.WebSearch.Extractor.LangExtractor import LangExtractor
from app.core.Agents.Poi.PoiMapper.GoogleMapsPoiMapper import GoogleMapsPoiMapper


class PoiGraph:
    """
    POI ê²€ìƒ‰ ì›Œí¬í”Œë¡œìš° ê·¸ë˜í”„ (VectorDB-first ì „ëµ)

    í˜ë¥´ì†Œë‚˜ ê¸°ë°˜ìœ¼ë¡œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ê³ ,
    VectorDBë¥¼ ìš°ì„  ì¡°íšŒí•˜ì—¬ ê´€ë ¨ë„ ë†’ì€ ê²°ê³¼ë¥¼ ë¨¼ì € í™•ë³´í•©ë‹ˆë‹¤.
    VectorDB ê²°ê³¼ê°€ ì¶©ë¶„í•˜ë©´ ì›¹ ê²€ìƒ‰ì„ ìŠ¤í‚µí•˜ê³ ,
    ë¶€ì¡±í•œ ê²½ìš°ì—ë§Œ ì›¹ ê²€ìƒ‰ì„ ì¶”ê°€ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.
    """
    
    def __init__(
        self,
        llm_client: BaseLLMClient,
        rerank_min_score: float, # ë¦¬ë­ì»¤ì—ì„œ ìµœì†Œ ì ìˆ˜ ì„ê³„ê°’ (ì´í•˜ëŠ” ë²„ë¦¼)
        keyword_k: int,     # í‚¤ì›Œë“œ ì¶”ì¶œì—ì„œ ìƒìœ„ Nê°œë¥¼ ì„ íƒ ê·¸ ì´í•˜ëŠ” ë²„ë¦¼
        embedding_k: int,   # ì„ë² ë”© ê²€ìƒ‰ì—ì„œ ìƒìœ„ Nê°œë¥¼ ì„ íƒ ê·¸ ì´í•˜ëŠ” ë²„ë¦¼
        web_search_k: int,  # ì›¹ ê²€ìƒ‰ì—ì„œ ìƒìœ„ Nê°œë¥¼ ì„ íƒ ê·¸ ì´í•˜ëŠ” ë²„ë¦¼
        final_poi_count: int, # ìµœì¢… POI ê°œìˆ˜
        vector_db_path: Optional[str] = None,  # Noneì´ë©´ app/data/vector_db/ ì‚¬ìš©
        web_weight: float = 0.6,
        embedding_weight: float = 0.4,
        vector_search_agent: Optional[BaseVectorSearchAgent] = None,
    ):
        """
        ì´ ê²€ìƒ‰ ê°¯ìˆ˜ëŠ” keyword_k * web_search_k ì…ë‹ˆë‹¤.
        ê²°ê³¼ëŠ” ì¥ë‹´ ë°›ì„ ìˆ˜ ì—†ìŒ
        langextractorë¡œ ê²°ê³¼ê°€ ê°€ì ¸ì˜¨ ê°¯ìˆ˜ ë§Œí¼ Mapping 
        """
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.keyword_extractor = QueryExtension(llm_client)
        self.extractor = LangExtractor()
        self.jina_reader = JinaReader()
        # Google Maps API í‚¤ ì„¤ì •
        self.web_search = WebSearchAgent(
            extractor=self.extractor,
            jina_reader=self.jina_reader,
            num_results=web_search_k
        )

        if vector_search_agent is not None:
            self.vector_search = vector_search_agent
            self.embedding_pipeline = vector_search_agent.embedding_pipeline
        else:
            self.embedding_pipeline = EmbeddingPipeline()
            self.vector_search = VectorSearchAgent(
                embedding_pipeline=self.embedding_pipeline,
                persist_directory=vector_db_path
            )
        self.result_merger = ResultMerger(
            web_weight=web_weight,
            embedding_weight=embedding_weight
        )
        self.info_summarizer = InfoSummarizeAgent(llm_client)
        self.reranker = Reranker(llm_client, min_score=rerank_min_score)
        self.alias_cache = PoiAliasCache()
        self.keyword_k = keyword_k
        self.web_search_k = web_search_k
        self.embedding_k = embedding_k
        self.final_poi_count = final_poi_count
        
        # POI Mapper ì´ˆê¸°í™” (Google Maps API ê²€ì¦ìš©)
        self.poi_mapper: Optional[BasePoiMapper] = GoogleMapsPoiMapper()
        
        # í†µê³„ ì¶”ì ìš©
        self._stats: Optional[PoiSearchStats] = None
        
        # ê·¸ë˜í”„ ë¹Œë“œ
        self.graph = self._build_graph()
    
    def _build_graph(self):
        """LangGraph ì›Œí¬í”Œë¡œìš° ë¹Œë“œ

        Flow Architecture (VectorDB-first):
        1. extract_keywords: í˜ë¥´ì†Œë‚˜ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
        2. vector_db_first_search: VectorDB ìš°ì„  ì¡°íšŒ (ê´€ë ¨ë„ >= 0.9)
        3. rerank_embedding: VectorDB ê²°ê³¼ ë¦¬ë­í‚¹
        4. [conditional] _check_poi_sufficiency:
           - sufficient (>= final_poi_count): â†’ merge_results â†’ END
           - insufficient (< final_poi_count): â†’ web_search â†’ process_and_rerank_web â†’ merge_results â†’ END
        """
        workflow = StateGraph(PoiAgentState)

        # ë…¸ë“œ ì¶”ê°€
        workflow.add_node("extract_keywords", self._extract_keywords)
        workflow.add_node("vector_db_first_search", self._vector_db_first_search)
        workflow.add_node("rerank_embedding", self._rerank_embedding)
        workflow.add_node("web_search", self._web_search)
        workflow.add_node("process_and_rerank_web", self._process_and_rerank_web)
        workflow.add_node("merge_results", self._merge_results)

        # ìˆœì°¨ íë¦„: í‚¤ì›Œë“œ â†’ VectorDB ì¡°íšŒ â†’ ë¦¬ë­í‚¹
        workflow.set_entry_point("extract_keywords")
        workflow.add_edge("extract_keywords", "vector_db_first_search")
        workflow.add_edge("vector_db_first_search", "rerank_embedding")

        # ë¦¬ë­í‚¹ í›„ ì¡°ê±´ ë¶„ê¸°
        workflow.add_conditional_edges(
            "rerank_embedding",
            self._check_poi_sufficiency,
            {
                "sufficient": "merge_results",
                "insufficient": "web_search"
            }
        )

        # ì›¹ ê²€ìƒ‰ ê²½ë¡œ
        workflow.add_edge("web_search", "process_and_rerank_web")
        workflow.add_edge("process_and_rerank_web", "merge_results")

        # ë³‘í•© í›„ ì¢…ë£Œ
        workflow.add_edge("merge_results", END)

        return workflow.compile()
    
    async def _extract_keywords(self, state: PoiAgentState) -> dict:
        """í˜ë¥´ì†Œë‚˜ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ ë…¸ë“œ"""
        keywords = await self.keyword_extractor.extract_keywords(
            persona_summary=state["persona_summary"],
            destination=state["travel_destination"],
            start_date=state["start_date"],
            end_date=state["end_date"]
        )
        logger.info(f"í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ: {len(keywords)}ê°œ í‚¤ì›Œë“œ")
        logger.info(f"ì¶”ì¶œëœ í‚¤ì›Œë“œ: {keywords}")
        return {"keywords": keywords}
    
    @staticmethod
    def _normalize_poi_name(name: str) -> str:
        """POI ì´ë¦„ ì •ê·œí™” (ì¤‘ë³µ ë¹„êµìš©)
        
        - ì—°ì† ê³µë°±ì„ ë‹¨ì¼ ê³µë°±ìœ¼ë¡œ
        - ì–‘ìª½ ê³µë°± ì œê±°
        - ì†Œë¬¸ì ë³€í™˜
        """
        if not name:
            return ""
        normalized = re.sub(r'\s+', ' ', name.strip())
        return normalized.lower()

    async def _web_search(self, state: PoiAgentState) -> dict:
        """ì›¹ ê²€ìƒ‰ ë…¸ë“œ"""
        keywords = state.get("keywords", [])
        if not keywords:
            return {"web_results": []}

        logger.info(f"ì›¹ ê²€ìƒ‰ ì‹œì‘: {self.keyword_k}ê°œ í‚¤ì›Œë“œ ì‚¬ìš©")
        logger.info(f"ì›¹ ê²€ìƒ‰ í‚¤ì›Œë“œ: {keywords[:self.keyword_k]}")
        travel_destination = state.get("travel_destination", "")
        
        # stats ê°ì²´ ì „ë‹¬í•˜ì—¬ í†µê³„ ìˆ˜ì§‘
        results = await self.web_search.search_multiple(
            keywords[:self.keyword_k],
            destination=travel_destination,
            stats=self._stats
        )

        # === 1ì°¨ ì¤‘ë³µ ì œê±°: title ê¸°ì¤€ (LangExtractor ê²°ê³¼ ë‚´ ì¤‘ë³µ) ===
        seen_titles: set = set()
        unique_results: List[PoiSearchResult] = []
        for result in results:
            normalized_title = self._normalize_poi_name(result.title)
            if normalized_title not in seen_titles:
                seen_titles.add(normalized_title)
                unique_results.append(result)
            else:
                logger.info(f"1ì°¨ ì¤‘ë³µ ì œê±° (title): {result.title}")

        for result in unique_results:
            logger.info(f"ì›¹ ê²€ìƒ‰ ê²°ê³¼: {result.title}")
        
        logger.info(f"1ì°¨ ì¤‘ë³µ ì œê±°: {len(results)}ê°œ -> {len(unique_results)}ê°œ")
        
        # í†µê³„ ìˆ˜ì§‘: ì „ì²´ POI í†µê³„ (ì›¹ ê²€ìƒ‰)
        if self._stats is not None:
            self._stats["web_raw_poi_count"] = len(results)
            self._stats["web_dup_removed"] = len(results) - len(unique_results)
            self._stats["web_final_poi_count"] = len(unique_results)
        
        return {"web_results": unique_results}

    async def _process_and_rerank_web(self, state: PoiAgentState) -> dict:
        """
        ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°°ì¹˜ë¡œ ì²˜ë¦¬ + ë¦¬ë­í‚¹í•˜ëŠ” í†µí•© ë…¸ë“œ.

        10ê°œì”© ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´:
        1. ê°œë³„ POI ìš”ì•½ (InfoSummarizeAgent)
        2. Google Maps ê²€ì¦ (PoiMapper)
        3. VectorDB ì €ì¥
        4. ë¦¬ë­í‚¹
        5. relevance_score >= 0.5ì¸ ê²°ê³¼ê°€ 20ê°œ ì´ìƒì´ë©´ ì¡°ê¸° ì¢…ë£Œ
        """
        web_results = state.get("web_results", [])
        persona_summary = state["persona_summary"]
        travel_destination = state["travel_destination"]

        if not web_results:
            return {"reranked_web_results": [], "poi_data_map": {}}

        BATCH_SIZE = 10
        MIN_SCORE = 0.5
        travel_days = state.get("travel_days", 0)
        TARGET_COUNT = travel_days * 10 if travel_days > 0 else 20

        all_reranked: List[PoiSearchResult] = []
        all_poi_data: Dict[str, PoiData] = {}
        semaphore = asyncio.Semaphore(5)
        
        # í†µê³„ ì¶”ì ìš© ì¹´ìš´í„°
        vectordb_hit_count = 0
        mapper_processed_count = 0
        total_checked = 0
        summarize_failed_count = 0
        mapper_failed_count = 0
        other_error_count = 0
        rerank_pre_count = 0
        rerank_post_count = 0
        rerank_dropped_items: List[tuple] = []  # (title, score)

        for batch_start in range(0, len(web_results), BATCH_SIZE):
            batch = web_results[batch_start:batch_start + BATCH_SIZE]
            logger.info(f"ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘: {batch_start + 1}~{batch_start + len(batch)} / {len(web_results)}")

            # --- 1) ë°°ì¹˜ ë‚´ ê°œë³„ POI ì²˜ë¦¬ ---
            processed_batch: List[PoiSearchResult] = []
            batch_poi_data: List[PoiData] = []

            async def process_single_poi(poi_result: PoiSearchResult) -> Optional[tuple]:
                """Returns (PoiSearchResult, PoiData, is_vectordb_hit) or error tuple"""
                async with semaphore:
                    try:
                        poi_info = await self.info_summarizer.summarize_single(
                            poi_result=poi_result,
                            persona_summary=persona_summary
                        )
                        if not poi_info:
                            logger.warning(f"POI ìš”ì•½ ì‹¤íŒ¨ (summarize_single): {poi_result}")
                            return ("SUMMARIZE_FAILED", None, None)

                        normalized_name = self._normalize_poi_name(poi_info.name)

                        # === 1ë‹¨ê³„: ë³„ì¹­ ìºì‹œì—ì„œ ì´ë¦„ ì¡°íšŒ ===
                        cached_place_id = await self.alias_cache.find_by_name(
                            normalized_name, travel_destination
                        )
                        if cached_place_id:
                            existing_poi = await self.vector_search.find_by_google_place_id(
                                cached_place_id, city_filter=travel_destination
                            )
                            if existing_poi:
                                logger.info(f"ë³„ì¹­ ìºì‹œ íˆíŠ¸: {poi_info.name} â†’ place_id={cached_place_id}")
                                search_result = PoiSearchResult(
                                    poi_id=existing_poi.id,
                                    title=poi_result.title,
                                    snippet=poi_result.snippet,
                                    url=poi_result.url,
                                    source=poi_result.source,
                                    relevance_score=poi_result.relevance_score
                                )
                                return (search_result, existing_poi, True)

                        # === 2ë‹¨ê³„: Mapper í˜¸ì¶œ ===
                        try:
                            poi_data = await self.poi_mapper.map_poi(
                                poi_info=poi_info,
                                city=travel_destination,
                                source_url=poi_result.url,
                                raise_on_failure=True
                            )
                        except PoiValidationError as e:
                            logger.warning(f"POI ê²€ì¦ ì‹¤íŒ¨(Google Maps): {e}")
                            logger.warning(f"             {poi_result}")
                            return ("MAPPER_FAILED", None, None)

                        if not poi_data:
                            return ("MAPPER_FAILED", None, None)

                        # === 3ë‹¨ê³„: Mapper ê²°ê³¼ì˜ place_idë¡œ ë³„ì¹­ í™•ì¸ ===
                        if poi_data.google_place_id:
                            is_alias = await self.alias_cache.has_place_id(
                                poi_data.google_place_id
                            )
                            if is_alias:
                                # ë‹¤ë¥¸ ì´ë¦„ì˜ ê°™ì€ ì¥ì†Œ â†’ ë³„ì¹­ ë“±ë¡
                                logger.info(f"ë³„ì¹­ ê°ì§€: {poi_info.name} â†’ ê¸°ì¡´ place_id={poi_data.google_place_id}")
                                await self.alias_cache.add(
                                    normalized_name, travel_destination,
                                    poi_data.google_place_id
                                )
                                existing_poi = await self.vector_search.find_by_google_place_id(
                                    poi_data.google_place_id
                                )
                                if existing_poi:
                                    search_result = PoiSearchResult(
                                        poi_id=existing_poi.id,
                                        title=poi_result.title,
                                        snippet=poi_result.snippet,
                                        url=poi_result.url,
                                        source=poi_result.source,
                                        relevance_score=poi_result.relevance_score
                                    )
                                    return (search_result, existing_poi, True)

                            # ìƒˆ POI â†’ ë³„ì¹­ ìºì‹œì— ë“±ë¡
                            await self.alias_cache.add(
                                normalized_name, travel_destination,
                                poi_data.google_place_id
                            )

                        search_result = PoiSearchResult(
                            poi_id=poi_data.id,
                            title=poi_result.title,
                            snippet=poi_result.snippet,
                            url=poi_result.url,
                            source=poi_result.source,
                            relevance_score=poi_result.relevance_score
                        )
                        return (search_result, poi_data, False)  # Mapper ì²˜ë¦¬
                    except Exception as e:
                        logger.error(f"POI ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {poi_result.title} - {e}")
                        return ("OTHER_ERROR", None, None)

            tasks = [process_single_poi(r) for r in batch]
            results = await asyncio.gather(*tasks, return_exceptions=True)

            for result in results:
                if isinstance(result, tuple) and len(result) == 3:
                    error_code, data1, data2 = result
                    if error_code == "SUMMARIZE_FAILED":
                        summarize_failed_count += 1
                    elif error_code == "MAPPER_FAILED":
                        mapper_failed_count += 1
                    elif error_code == "OTHER_ERROR":
                        other_error_count += 1
                    elif isinstance(error_code, PoiSearchResult):
                        # ì„±ê³µ: (PoiSearchResult, PoiData, is_vectordb_hit)
                        processed_batch.append(error_code)
                        batch_poi_data.append(data1)
                        if data2:  # is_vectordb_hit
                            vectordb_hit_count += 1
                        else:
                            mapper_processed_count += 1
                elif isinstance(result, Exception):
                    logger.error(f"POI ì²˜ë¦¬ ì˜ˆì™¸: {result}")
                    other_error_count += 1
            
            total_checked += len(batch)

            # VectorDB ì €ì¥
            if batch_poi_data:
                try:
                    await self.vector_search.add_pois_batch(batch_poi_data)
                    logger.info(f"VectorDB ì €ì¥ ì™„ë£Œ: {len(batch_poi_data)}ê°œ POI")
                except Exception as e:
                    logger.error(f"VectorDB ì €ì¥ ì‹¤íŒ¨: {e}")

            for pd in batch_poi_data:
                all_poi_data[pd.id] = pd

            # --- 2) ë°°ì¹˜ ë¦¬ë­í‚¹ ---
            if processed_batch:
                rerank_pre_count += len(processed_batch)
                batch_dropped: list = []
                reranked_batch = await self.reranker.rerank(
                    processed_batch, persona_summary, dropped_out=batch_dropped
                )
                rerank_post_count += len(reranked_batch)
                rerank_dropped_items.extend(batch_dropped)

                all_reranked.extend(reranked_batch)

            # --- 3) ì¡°ê¸° ì¢…ë£Œ ê²€ì‚¬ ---
            good_count = sum(1 for r in all_reranked if r.relevance_score >= MIN_SCORE)
            logger.info(f"ë°°ì¹˜ ì™„ë£Œ: ëˆ„ì  {len(all_reranked)}ê°œ, ì–‘í˜¸(>={MIN_SCORE}) {good_count}ê°œ")

            if good_count >= TARGET_COUNT:
                logger.info(f"ëª©í‘œ ë‹¬ì„± ({good_count}>={TARGET_COUNT}), ì¡°ê¸° ì¢…ë£Œ")
                # í†µê³„: ì¡°ê¸° ì¢…ë£Œë¡œ ìŠ¤í‚µëœ POI ìˆ˜
                if self._stats is not None:
                    remaining = len(web_results) - (batch_start + len(batch))
                    self._stats["early_termination_checked"] = total_checked
                    self._stats["early_termination_skipped"] = remaining
                break

        all_reranked.sort(key=lambda x: x.relevance_score, reverse=True)
        logger.info(f"ì›¹ ê²°ê³¼ ì²˜ë¦¬+ë¦¬ë­í‚¹ ì™„ë£Œ: {len(all_reranked)}ê°œ (ì „ì²´ {len(web_results)}ê°œ ì¤‘)")
        
        # í†µê³„ ìˆ˜ì§‘: VectorDB íˆíŠ¸ vs Mapper ì²˜ë¦¬
        if self._stats is not None:
            self._stats["vectordb_hit_count"] = vectordb_hit_count
            self._stats["mapper_processed_count"] = mapper_processed_count
            # ì‹¤íŒ¨ í†µê³„
            self._stats["summarize_failed_count"] = summarize_failed_count
            self._stats["mapper_failed_count"] = mapper_failed_count
            self._stats["other_error_count"] = other_error_count
            # ë¦¬ë­ì»¤ íƒˆë½ í†µê³„
            self._stats["rerank_pre_count"] = rerank_pre_count
            self._stats["rerank_post_count"] = rerank_post_count
            self._stats["rerank_dropped_count"] = rerank_pre_count - rerank_post_count
            self._stats["rerank_dropped_items"] = rerank_dropped_items
            # ì¡°ê¸° ì¢…ë£Œê°€ ì—†ì—ˆìœ¼ë©´ ì „ì²´ ê²€ì‚¬
            if "early_termination_checked" not in self._stats:
                self._stats["early_termination_checked"] = total_checked
                self._stats["early_termination_skipped"] = 0
        
        return {"reranked_web_results": all_reranked, "poi_data_map": all_poi_data}

    async def _vector_db_first_search(self, state: PoiAgentState) -> dict:
        """VectorDB ìš°ì„  ì¡°íšŒ ë…¸ë“œ

        VectorDBì—ì„œ ê´€ë ¨ë„ 0.55 ì´ìƒì¸ ê²°ê³¼ë§Œ í•„í„°ë§í•˜ì—¬ ë°˜í™˜.
        travel_destinationì´ ìˆìœ¼ë©´ í•´ë‹¹ ë„ì‹œë¡œ í•„í„°ë§í•˜ì—¬ ê²€ìƒ‰.
        metadataì—ì„œ PoiDataë¥¼ ë³µì›í•˜ì—¬ poi_data_mapì— ì¶”ê°€.
        """
        RELEVANCE_THRESHOLD = 0.3

        keywords = state.get("keywords", [])
        travel_destination = state.get("travel_destination", "")

        logger.info(f"VectorDB ìš°ì„  ì¡°íšŒ ì‹œì‘: {len(keywords)}ê°œ í‚¤ì›Œë“œ ì‚¬ìš©")

        all_pairs: list = []
        pairs = await self.vector_search.search_by_text_with_data(
            query=state.get("persona_summary", ", ".join(keywords)),
            k=self.embedding_k,
            city_filter=travel_destination
        )
        all_pairs.extend(pairs)

        # ì¤‘ë³µ ì œê±° (poi_id ê¸°ì¤€) + ê´€ë ¨ë„ threshold í•„í„°ë§
        seen_ids: set = set()
        filtered_results: List[PoiSearchResult] = []
        embedding_poi_data_map: Dict[str, PoiData] = {}
        for search_result, poi_data in all_pairs:
            if search_result.poi_id and search_result.poi_id not in seen_ids:
                seen_ids.add(search_result.poi_id)
                if search_result.relevance_score >= RELEVANCE_THRESHOLD:
                    filtered_results.append(search_result)
                    embedding_poi_data_map[search_result.poi_id] = poi_data

        logger.info(f"VectorDB ìš°ì„  ì¡°íšŒ: {len(filtered_results)}ê°œ (ê´€ë ¨ë„ >= {RELEVANCE_THRESHOLD})")

        for poi in filtered_results:
            logger.info(f"VectorDB ìš°ì„  ì¡°íšŒ: {poi.title} ({poi.relevance_score})")
        
        # í†µê³„ ìˆ˜ì§‘: ì„ë² ë”© ê²€ìƒ‰ POI ê°œìˆ˜
        if self._stats is not None:
            self._stats["embedding_poi_count"] = len(filtered_results)

        return {
            "embedding_results": filtered_results,
            "poi_data_map": embedding_poi_data_map
        }
    
    async def _rerank_embedding(self, state: PoiAgentState) -> dict:
        """ì„ë² ë”© ê²€ìƒ‰ ê²°ê³¼ ë¦¬ë­í‚¹ ë…¸ë“œ"""
        embedding_results = state.get("embedding_results", [])
        persona_summary = state["persona_summary"]
        reranked = []

        logger.info(f"rerank_embedding ì…ë ¥: {len(embedding_results)}ê°œ")
        for i in range(0, len(embedding_results), 5):
            reranked.extend(await self.reranker.rerank(embedding_results[i:i+5], persona_summary))

        reranked.sort(key=lambda x: x.relevance_score, reverse=True)
        logger.info(f"rerank_embedding ì¶œë ¥: {len(reranked)}ê°œ")
        return {"reranked_embedding_results": reranked}

    def _check_poi_sufficiency(self, state: PoiAgentState) -> str:
        """VectorDB ë¦¬ë­í‚¹ ê²°ê³¼ê°€ ì¶©ë¶„í•œì§€ íŒì •í•˜ëŠ” ì¡°ê±´ë¶€ ë¼ìš°í„°

        reranked_embedding_results ê°œìˆ˜ê°€ final_poi_count ì´ìƒì´ë©´ 'sufficient',
        ë¯¸ë§Œì´ë©´ 'insufficient'ë¥¼ ë°˜í™˜í•˜ì—¬ ì›¹ ê²€ìƒ‰ ì—¬ë¶€ë¥¼ ê²°ì •.
        """
        reranked = state.get("reranked_embedding_results", [])
        count = len(reranked)
        target = state.get("final_poi_count", self.final_poi_count)
        if count >= target:
            logger.info(f"POI ì¶©ë¶„: {count}ê°œ >= {target}ê°œ (ì›¹ ê²€ìƒ‰ ìŠ¤í‚µ)")
            return "sufficient"
        else:
            logger.info(f"POI ë¶€ì¡±: {count}ê°œ < {target}ê°œ (ì›¹ ê²€ìƒ‰ ì§„í–‰)")
            return "insufficient"

    async def _merge_results(self, state: PoiAgentState) -> dict:
        """ê²°ê³¼ ë³‘í•© ë…¸ë“œ (ë¦¬ë­í‚¹ëœ ê²°ê³¼ ì‚¬ìš©)

        ìƒˆ í”Œë¡œìš°ì—ì„œëŠ” ì›¹ ê²€ìƒ‰ ê²°ê³¼ê°€ ì´ë¯¸ process_web_resultsì—ì„œ
        PoiInfoë¡œ ë³€í™˜ë˜ê³  VectorDBì— ì €ì¥ëœ ìƒíƒœì…ë‹ˆë‹¤.
        ì—¬ê¸°ì„œëŠ” ì–‘ìª½ ê²°ê³¼ë¥¼ ë³‘í•©í•˜ê³ , poi_data_mapì—ì„œ PoiDataë¥¼ ì¡°íšŒí•˜ì—¬
        ìµœì¢… List[PoiData]ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        """
        web_reranked = state.get("reranked_web_results", [])
        emb_reranked = state.get("reranked_embedding_results", [])
        logger.info(f"merge ì…ë ¥ - ì›¹: {len(web_reranked)}ê°œ, ì„ë² ë”©: {len(emb_reranked)}ê°œ")

        merged = self.result_merger.merge(
            web_results=web_reranked,
            embedding_results=emb_reranked,
            stats=self._stats
        )
        logger.info(f"merge ì¶œë ¥ (ì¤‘ë³µ ì œê±° í›„): {len(merged)}ê°œ")
        
        # í†µê³„ ìˆ˜ì§‘: ë³‘í•© ì „í›„ POI ìˆ˜
        if self._stats is not None:
            self._stats["pre_merge_web_count"] = len(web_reranked)
            self._stats["pre_merge_embedding_count"] = len(emb_reranked)
            self._stats["post_merge_count"] = len(merged)

        # ìµœì¢… POI ê°œìˆ˜ ì œí•œ
        # merged = merged[:self.final_poi_count]

        # poi_data_mapì—ì„œ PoiData ì¡°íšŒí•˜ì—¬ ìµœì¢… ë¦¬ìŠ¤íŠ¸ ìƒì„±
        poi_data_map = state.get("poi_data_map", {})

        # === ì¤‘ë³µ POI ë³„ì¹­ DB ë“±ë¡ ===
        merge_dup_pairs = self._stats.get("merge_dup_pairs", []) if self._stats else []
        travel_destination = state.get("travel_destination", "")
        alias_registered_count = 0

        for pair in merge_dup_pairs:
            dup_title = pair.get("title", "")
            existing_poi_id = pair.get("poi_id", "")

            if not dup_title or not existing_poi_id:
                continue

            poi_data = poi_data_map.get(existing_poi_id)
            if poi_data and poi_data.google_place_id:
                await self.alias_cache.add(
                    dup_title, travel_destination, poi_data.google_place_id
                )
                logger.info(
                    f"ë³„ì¹­ DB ë“±ë¡ (merge ì¤‘ë³µ): {dup_title} â†’ {poi_data.google_place_id}"
                )
                alias_registered_count += 1

        if alias_registered_count > 0:
            logger.info(f"merge ë‹¨ê³„ì—ì„œ ë³„ì¹­ DBì— {alias_registered_count}ê°œ ë“±ë¡ ì™„ë£Œ")
        final_poi_data: List[PoiData] = []

        for result in merged:
            if result.poi_id and result.poi_id in poi_data_map:
                final_poi_data.append(poi_data_map[result.poi_id])
            else:
                logger.warning(f"PoiData not found for poi_id: {result.poi_id}, title: {result.title}")
        return {"merged_results": merged, "final_poi_data": final_poi_data}
    
    async def run(
        self,
        persona_summary: str,
        travel_destination: str,
        start_date: str = "",
        end_date: str = "",
        save_path: Optional[str] = None
    ) -> List[PoiData]:
        """
        POI ê²€ìƒ‰ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ (VectorDB-first ì „ëµ)

        Flow:
        1. í‚¤ì›Œë“œ ì¶”ì¶œ
        2. VectorDB ìš°ì„  ì¡°íšŒ (ê´€ë ¨ë„ >= 0.9 í•„í„°ë§)
        3. VectorDB ê²°ê³¼ ë¦¬ë­í‚¹
        4. ì¶©ë¶„ì„± íŒì • (>= final_poi_count)
           - ì¶©ë¶„: ë°”ë¡œ merge â†’ ìµœì¢… ê²°ê³¼
           - ë¶€ì¡±: ì›¹ ê²€ìƒ‰ â†’ ì›¹ ê²°ê³¼ ì²˜ë¦¬/ë¦¬ë­í‚¹ â†’ merge â†’ ìµœì¢… ê²°ê³¼
        5. poi_data_mapì—ì„œ PoiData ì¡°íšŒí•˜ì—¬ ìµœì¢… List[PoiData] ë°˜í™˜

        Args:
            persona_summary: ì‚¬ìš©ì í˜ë¥´ì†Œë‚˜ ìš”ì•½
            travel_destination: ì‚¬ìš©ìê°€ ì—¬í–‰í•˜ëŠ” ì§€ì—­
            start_date: ì—¬í–‰ ì‹œì‘ì¼ (YYYY-MM-DD)
            end_date: ì—¬í–‰ ì¢…ë£Œì¼ (YYYY-MM-DD)
            save_path: JSON ì €ì¥ ê²½ë¡œ (ì„ íƒì )

        Returns:
            ìµœì¢… POI ë°ì´í„° ëª©ë¡ (List[PoiData], Google Maps ê²€ì¦ ì™„ë£Œ)
        """
        logger.info(f"POI Graph ì‹¤í–‰ ì‹œì‘: {travel_destination}, {persona_summary[:50]}...")

        # í†µê³„ ì´ˆê¸°í™”
        self._stats: PoiSearchStats = {}

        # ì—¬í–‰ì¼ìˆ˜ ê³„ì‚° â†’ ë™ì  POI ê°¯ìˆ˜ ì„¤ì •
        travel_days = None
        if start_date and end_date:
            try:
                sd = datetime.strptime(start_date, "%Y-%m-%d")
                ed = datetime.strptime(end_date, "%Y-%m-%d")
                travel_days = (ed - sd).days + 1
                if travel_days < 1:
                    travel_days = None
            except ValueError:
                travel_days = None

        dynamic_poi_count = travel_days * 5 if travel_days else self.final_poi_count
        logger.info(f"ì—¬í–‰ì¼ìˆ˜={travel_days}, dynamic_poi_count={dynamic_poi_count}")

        initial_state: PoiAgentState = {
            "travel_destination": travel_destination,
            "persona_summary": persona_summary,
            "start_date": start_date,
            "end_date": end_date,
            "keywords": [],
            "web_results": [],
            "embedding_results": [],
            "reranked_web_results": [],
            "reranked_embedding_results": [],
            "merged_results": [],
            "poi_data_map": {},
            "final_poi_data": [],
            "final_pois": [],
            "final_poi_count": dynamic_poi_count,
            "travel_days": travel_days or 0,
        }

        result = await self.graph.ainvoke(initial_state)

        # ë””ë²„ê·¸ ë¡œê¹…
        for key, value in result.items():
            if isinstance(value, list):
                logger.info(f"{key}: {len(value)}ê°œ")
            elif isinstance(value, dict):
                logger.info(f"{key}: {len(value)}ê°œ")

        # ìµœì¢… ê²°ê³¼ëŠ” final_poi_data (List[PoiData])
        final_poi_data = result.get("final_poi_data", [])

        logger.info(f"POI Graph ì‹¤í–‰ ì™„ë£Œ: ì´ {len(final_poi_data)}ê°œ POI ë°˜í™˜")
        
        # í†µê³„ ë³´ê³ ì„œ ì¶œë ¥
        self._print_search_report()

        # JSON ì €ì¥ (ìš”ì²­ ì‹œ)
        if save_path:
            self.save_state_to_json(result, save_path)

        return final_poi_data, result
    
    @staticmethod
    def _serialize_item(item: Any) -> Any:
        """Pydantic ëª¨ë¸ê³¼ Enumì„ ì§ë ¬í™”"""
        if hasattr(item, 'model_dump'):
            # Pydantic ëª¨ë¸ -> dictë¡œ ë³€í™˜
            dumped = item.model_dump()
            # Enum ê°’ë“¤ì„ ì¬ê·€ì ìœ¼ë¡œ ì²˜ë¦¬
            return PoiGraph._serialize_item(dumped)
        elif hasattr(item, 'value'):  # Enum
            return item.value
        elif isinstance(item, list):
            return [PoiGraph._serialize_item(i) for i in item]
        elif isinstance(item, dict):
            return {k: PoiGraph._serialize_item(v) for k, v in item.items()}
        return item

    def save_state_to_json(self, state: PoiAgentState, file_path: str) -> bool:
        """
        PoiAgentState ì „ì²´ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
        
        Args:
            state: ì €ì¥í•  ì „ì²´ ìƒíƒœ
            file_path: ì €ì¥í•  íŒŒì¼ ê²½ë¡œ
        
        Returns:
            ì €ì¥ ì„±ê³µ ì—¬ë¶€
        """
        try:
            output = {
                "metadata": {
                    "generated_at": datetime.now().isoformat()
                },
                **{key: self._serialize_item(value) for key, value in state.items()}
            }
            
            Path(file_path).parent.mkdir(parents=True, exist_ok=True)
            
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(output, f, ensure_ascii=False, indent=2, default=str)
            
            logger.info(f"JSON ì €ì¥ ì„±ê³µ: {file_path}")
            return True
            
        except Exception as e:
            logger.error(f"JSON ì €ì¥ ì‹¤íŒ¨: {e}")
            return False

    def _print_search_report(self) -> None:
        """POI ê²€ìƒ‰ í†µê³„ ë³´ê³ ì„œë¥¼ ë¡œê·¸ë¡œ ì¶œë ¥"""
        if self._stats is None:
            logger.warning("í†µê³„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        s = self._stats
        
        # êµ¬ë¶„ì„ 
        separator = "=" * 80
        
        lines = [
            "",
            separator,
            "                        ğŸ“Š POI ê²€ìƒ‰ í†µê³„ ë³´ê³ ì„œ",
            separator,
            "",
            f"[0] ì„ë² ë”©(VectorDB) ê²€ìƒ‰ POI: {s.get('embedding_poi_count', 0)}ê°œ",
            "",
        ]
        
        # [1] ì›¹ ê²€ìƒ‰ í‚¤ì›Œë“œ
        keywords = s.get("keywords", [])
        lines.append(f"[1] ì›¹ ê²€ìƒ‰ í‚¤ì›Œë“œ: {s.get('keyword_count', len(keywords))}ê°œ")
        if keywords:
            lines.append(f"    - {', '.join(keywords)}")
        lines.append("")
        
        # [2] í‚¤ì›Œë“œë³„ ê²€ìƒ‰ëœ ì›¹ í˜ì´ì§€
        pages_per_keyword = s.get("pages_per_keyword", {})
        if pages_per_keyword:
            lines.append("[2] í‚¤ì›Œë“œë³„ ê²€ìƒ‰ëœ ì›¹ í˜ì´ì§€:")
            for kw, count in pages_per_keyword.items():
                lines.append(f"    - {kw}: {count}í˜ì´ì§€")
        else:
            lines.append("[2] í‚¤ì›Œë“œë³„ ê²€ìƒ‰ëœ ì›¹ í˜ì´ì§€: (ë°ì´í„° ì—†ìŒ)")
        lines.append("")
        
        # [3] ìºì‹œ ì²˜ë¦¬ í†µê³„
        cache_hit = s.get("cache_hit_pages", 0)
        total_pages = s.get("total_pages", 0)
        cache_percent = (cache_hit / total_pages * 100) if total_pages > 0 else 0
        lines.append(f"[3] ìºì‹œë¡œ ì²˜ë¦¬ëœ ì›¹ í˜ì´ì§€: {cache_hit}ê°œ / {total_pages}ê°œ ({cache_percent:.1f}%)")
        lines.append("")
        
        # [4] ì›¹ í˜ì´ì§€ë³„ ì¶”ì¶œ POI
        pages_poi_stats = s.get("pages_poi_stats", [])
        if pages_poi_stats:
            # ìƒíƒœë³„ ì¹´ìš´íŠ¸
            success_count = sum(1 for p in pages_poi_stats if p.get("status") == "success")
            cache_count = sum(1 for p in pages_poi_stats if p.get("status") == "cache")
            jina_failed_count = sum(1 for p in pages_poi_stats if p.get("status") == "jina_failed")
            empty_count = sum(1 for p in pages_poi_stats if p.get("status") == "empty")
            
            lines.append(f"[4] ì›¹ í˜ì´ì§€ë³„ ì¶”ì¶œ POI: (ì„±ê³µ {success_count}, ìºì‹œ {cache_count}, Jinaì‹¤íŒ¨ {jina_failed_count}, ë¹ˆê²°ê³¼ {empty_count})")
            for page in pages_poi_stats[:10]:  # ìµœëŒ€ 10ê°œê¹Œì§€ë§Œ í‘œì‹œ
                url_short = page["url"][:60] + "..." if len(page["url"]) > 60 else page["url"]
                status = page.get("status", "success")
                if status == "success":
                    lines.append(f"    - {url_short}")
                    lines.append(f"      ì›ë³¸ {page['raw_count']}ê°œ â†’ ì¤‘ë³µ {page['dup_count']}ê°œ â†’ ìµœì¢… {page['final_count']}ê°œ")
                elif status == "cache":
                    lines.append(f"    - {url_short} (ìºì‹œ)")
                    lines.append(f"      ìµœì¢… {page['final_count']}ê°œ")
                elif status == "jina_failed":
                    lines.append(f"    - {url_short} (Jina ì‹¤íŒ¨)")
                elif status == "empty":
                    lines.append(f"    - {url_short} (POI ì—†ìŒ)")
            if len(pages_poi_stats) > 10:
                lines.append(f"    ... ì™¸ {len(pages_poi_stats) - 10}ê°œ í˜ì´ì§€")
        else:
            lines.append("[4] ì›¹ í˜ì´ì§€ë³„ ì¶”ì¶œ POI: (ë°ì´í„° ì—†ìŒ)")
        lines.append("")
        
        # [5] ì „ì²´ POI í†µê³„ (ì›¹ ê²€ìƒ‰)
        web_raw = s.get("web_raw_poi_count", 0)
        web_dup = s.get("web_dup_removed", 0)
        web_final = s.get("web_final_poi_count", 0)
        lines.append("[5] ì „ì²´ POI (ì›¹ ê²€ìƒ‰):")
        lines.append(f"    - ì›ë³¸: {web_raw}ê°œ â†’ ì¤‘ë³µ ì œê±°: {web_dup}ê°œ â†’ ìµœì¢…: {web_final}ê°œ")
        lines.append("")
        
        # [6] ë³„ì¹­ ìºì‹œ vs Mapper ì²˜ë¦¬
        alias_hit = s.get("vectordb_hit_count", 0)  # ì‹¤ì œë¡œëŠ” ë³„ì¹­ ìºì‹œ íˆíŠ¸
        mapper = s.get("mapper_processed_count", 0)
        lines.append("[6] ë³„ì¹­ ìºì‹œ vs Mapper ì²˜ë¦¬:")
        lines.append(f"    - ë³„ì¹­ ìºì‹œ íˆíŠ¸ (Mapper ìŠ¤í‚µ): {alias_hit}ê°œ")
        lines.append(f"    - Mapper ì²˜ë¦¬: {mapper}ê°œ")
        lines.append("")
        
        # [7] ì¡°ê¸° ì¢…ë£Œ í†µê³„
        early_checked = s.get("early_termination_checked", 0)
        early_skipped = s.get("early_termination_skipped", 0)
        lines.append("[7] ì¡°ê¸° ì¢…ë£Œ:")
        lines.append(f"    - ê²€ì‚¬í•œ POI: {early_checked}ê°œ")
        lines.append(f"    - ì¡°ê¸° ì¢…ë£Œë¡œ ìŠ¤í‚µ: {early_skipped}ê°œ")
        lines.append("")
        
        # [7-1] POI ì²˜ë¦¬ ì‹¤íŒ¨ í†µê³„
        summarize_failed = s.get("summarize_failed_count", 0)
        mapper_failed = s.get("mapper_failed_count", 0)
        other_error = s.get("other_error_count", 0)
        total_failed = summarize_failed + mapper_failed + other_error
        total_success = s.get("vectordb_hit_count", 0) + s.get("mapper_processed_count", 0)
        lines.append(f"[7-1] POI ì²˜ë¦¬ ì‹¤íŒ¨ í†µê³„: ì´ {total_failed}ê°œ íƒˆë½")
        lines.append(f"    - ìš”ì•½ ì‹¤íŒ¨: {summarize_failed}ê°œ")
        lines.append(f"    - Google Maps ê²€ì¦ ì‹¤íŒ¨: {mapper_failed}ê°œ")
        lines.append(f"    - ê¸°íƒ€ ì˜¤ë¥˜: {other_error}ê°œ")
        lines.append(f"    - ì„±ê³µ: {total_success}ê°œ")
        lines.append("")

        # [7-2] ë¦¬ë­ì»¤ íƒˆë½ í†µê³„
        rerank_pre = s.get("rerank_pre_count", 0)
        rerank_post = s.get("rerank_post_count", 0)
        rerank_dropped = s.get("rerank_dropped_count", 0)
        rerank_dropped_items = s.get("rerank_dropped_items", [])
        lines.append(f"[7-2] ë¦¬ë­ì»¤ í•„í„°ë§ (min_score ë¯¸ë§Œ íƒˆë½):")
        lines.append(f"    - ë¦¬ë­í‚¹ ì „: {rerank_pre}ê°œ â†’ ë¦¬ë­í‚¹ í›„: {rerank_post}ê°œ (íƒˆë½: {rerank_dropped}ê°œ)")
        if rerank_dropped_items:
            lines.append(f"    - íƒˆë½ POI ëª©ë¡:")
            for name, score in rerank_dropped_items:
                lines.append(f"      â€¢ {name} (ì ìˆ˜: {score:.2f})")
        lines.append("")

        # [8] ë³‘í•© ì „í›„
        pre_web = s.get("pre_merge_web_count", 0)
        pre_emb = s.get("pre_merge_embedding_count", 0)
        post_merge = s.get("post_merge_count", 0)
        lines.append("[8] ë³‘í•© ì „í›„:")
        lines.append(f"    - ì›¹ ê²€ìƒ‰: {pre_web}ê°œ")
        lines.append(f"    - ì„ë² ë”© ê²€ìƒ‰: {pre_emb}ê°œ")
        lines.append(f"    - ìµœì¢… ë³‘í•©: {post_merge}ê°œ")
        lines.append("")
        
        # [8-1] ë³‘í•© ì¤‘ë³µ ì œê±° ìƒì„¸
        merge_web_dup = s.get("merge_web_dup_count", 0)
        merge_emb_dup = s.get("merge_emb_dup_count", 0)
        merge_total_dup = s.get("merge_total_dup_count", 0)
        merge_web_dup_names = s.get("merge_web_dup_names", [])
        merge_emb_dup_names = s.get("merge_emb_dup_names", [])
        if merge_total_dup > 0:
            lines.append(f"[8-1] ë³‘í•© ì¤‘ë³µ ì œê±°: ì´ {merge_total_dup}ê°œ (ì ìˆ˜ í•©ì‚°)")
            if merge_web_dup > 0:
                lines.append(f"    - ì›¹ ê²€ìƒ‰ ë‚´ ì¤‘ë³µ (poi_id ê¸°ì¤€): {merge_web_dup}ê°œ")
                for name in merge_web_dup_names[:5]:  # ìµœëŒ€ 5ê°œê¹Œì§€
                    lines.append(f"      â€¢ {name}")
                if len(merge_web_dup_names) > 5:
                    lines.append(f"      ... ì™¸ {len(merge_web_dup_names) - 5}ê°œ")
            if merge_emb_dup > 0:
                lines.append(f"    - ì›¹â†”ì„ë² ë”© ì¤‘ë³µ: {merge_emb_dup}ê°œ")
                for name in merge_emb_dup_names[:5]:  # ìµœëŒ€ 5ê°œê¹Œì§€
                    lines.append(f"      â€¢ {name}")
                if len(merge_emb_dup_names) > 5:
                    lines.append(f"      ... ì™¸ {len(merge_emb_dup_names) - 5}ê°œ")
            lines.append("")
        
        lines.append(separator)
        lines.append("")
        
        # ë¡œê±°ë¡œ ì¶œë ¥
        for line in lines:
            logger.info(line)
